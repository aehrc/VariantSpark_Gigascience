# VariantSpark_Gigascience

## Createing an EMR clustr

we use [CreateCluster.sh](scripts/CreateCluster.sh) bash scrtip to create an EMR cluster using aws-cli. Te generated cluster has the following characteristics. **You are responsible to terminate your cluster when once you finish your process**. We have not created cloudformation template for this cluster yet.

- Use Spot pricing
- Use Uniform Instance
- Without EC2 keypair
- Master EC2 instance type: **r4.2xlarge** 8 vCPUs 61GB RAM
- Core EC2 instance type: **r4.4xlarge** 16 vCPUs 122GB RAM

If you would like to change the above configuration (i.e. if you want to use OnDemand pricing or SpotFleet), you may create this cluster and then clone it in the aws console and change the parameter there.

**Parameters:** You should modify the script and manually change these parameters:

- ClusterName: name of the cluster (i.e. C64 or C512)
- InstanceCount: Number of core instances (i.e. 4 or 32)
- LogURI: Path to S3 folder (or bucket) to store EMR logs

**Important**

You should install and configure awscli v2 on your machine (https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html). The above bash script uses _aws2_ (awscli v2) to create the EMR cluster. The _aws_ v1 command throw an error on BidPrice.

## VSdata: Syntetic genotype and phenotype generated by VariantSpark simulation module

We have used [SimulateData.sh](scripts/VSdata/SimulateData.sh) to create synthetic genotypes and phenotype using VariantSpark _gen-features_ and _gen-labels_ commands.

We have consider diffeerent number of samples and SNPs in the dataset.
-Samples: 1,000 - 10,000 - 100,000 (1K, 10K, 100K)
-SNPs: 100,000 - 1,000,000 - 10,000,000 - 100,000,000 (100K, 1M, 10M, 100M)

we ignore the dataset with 100K samples and 100M snps as its size were exteremly large.

Unfortunately _gen-features_ and _gen-labels_ does not work in the latest version of VariantSpark. Follow the instruction here: https://github.com/aehrc/VIGWAS/blob/master/Instructions/AWS_Instruction.pdf to create an EC2 instance with proper version of VariantSpark installed use r4.16xlarge instance and allocage 1000GB EBS volume. Then run this script on that EC2 instance.

**Parameters:** You should modify the script and manually change this parameter:

-S3: Path to S3 folder (or bucket) to store results.

Genotype data are generated using VariantSpark _gen-features_ command in _parquet_ format.

- SNP id: v_0 ... v_n
- Sample id: s_0 ... s_m

VariantSpark importance analysis (RandomForest Training) is slow when using parquet format (due to parallelisation problem). We have converted parquet files to csv.bz2 files (comma separated and bzip2 compressed). See description below for details.

Phenotype data are generated using VariantSpark _gen-lable_ command in csv format. The phenotype is simulated based on 5 rendomly selected SNPs (truth SNPs) with equal weight (=1) and a noise variable with mean=0.5 (-gm) and standard deviation=0.5 (-gs)

The phenotype file columns are:

- "":Sample Name
- "label": Binary phenotype (0 or 1)
- "pheno": continues phenotype
- The next 5 columns include genotypes of the truth SNPs for all samples where the column name is the SNP id

The script also store all simulation logs to the S3 path.

## VSdata: convert parquet file to csv.bz2
